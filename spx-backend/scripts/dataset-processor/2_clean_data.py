#!/usr/bin/env python3
"""
æ­¥éª¤2: æ•°æ®æ¸…æ´—å’Œç­›é€‰ï¼Œè¿‡æ»¤å‡ºé€‚åˆSPXå¹³å°çš„è½»é‡åŒ–ç´ æ
"""

import json
import re
import os
from pathlib import Path
from typing import List, Dict, Any, Set
from collections import Counter
import pandas as pd
from tqdm import tqdm

class DataCleaner:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.spx_keywords = set(keyword.lower() for keyword in self.config['data_filtering']['spx_keywords'])
        self.exclude_keywords = set(keyword.lower() for keyword in self.config['data_filtering']['exclude_keywords'])
        self.min_length = self.config['data_filtering']['min_name_length']
        self.max_length = self.config['data_filtering']['max_name_length']
        
        self.data_dir = Path("data")
        self.cleaned_dir = self.data_dir / "cleaned"
        self.cleaned_dir.mkdir(parents=True, exist_ok=True)

    def load_raw_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½åŸå§‹æ•°æ®"""
        raw_data_path = self.data_dir / "raw" / "raw_asset_data.json"
        
        if not raw_data_path.exists():
            print(f"âŒ Raw data file not found: {raw_data_path}")
            print("Please run 1_fetch_datasets.py first")
            return []
        
        with open(raw_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“¥ Loaded {len(data)} raw asset entries")
        return data

    def clean_asset_names(self, raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¸…æ´—ç´ æåç§°"""
        print("ğŸ§¹ Cleaning asset names...")
        
        cleaned_assets = []
        
        for item in tqdm(raw_data, desc="Cleaning assets"):
            name = item.get('name', '').strip()
            
            if not name:
                continue
            
            # åŸºç¡€æ¸…æ´—
            cleaned_name = self._basic_clean(name)
            if not cleaned_name:
                continue
            
            # SPXé€‚é…æ€§æ£€æŸ¥
            if not self._is_spx_compatible(cleaned_name):
                continue
            
            # å»é‡å’Œè§„èŒƒåŒ–
            normalized_name = self._normalize_name(cleaned_name)
            
            cleaned_item = {
                'name': normalized_name,
                'original_name': name,
                'source_dataset': item.get('source_dataset', ''),
                'source_url': item.get('source_url', ''),
                'category': item.get('category', ''),
                'confidence_score': self._calculate_confidence_score(normalized_name)
            }
            
            cleaned_assets.append(cleaned_item)
        
        print(f"âœ… Cleaned {len(cleaned_assets)} assets from {len(raw_data)} raw entries")
        return cleaned_assets

    def _basic_clean(self, name: str) -> str:
        """åŸºç¡€æ¸…æ´—æ“ä½œ"""
        if not isinstance(name, str):
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        name = re.sub(r'<[^>]+>', '', name)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦
        name = re.sub(r'[^\w\s\-\u4e00-\u9fff]', ' ', name)
        
        # åˆå¹¶å¤šä¸ªç©ºæ ¼
        name = re.sub(r'\s+', ' ', name).strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(name) < self.min_length or len(name) > self.max_length:
            return ""
        
        return name

    def _is_spx_compatible(self, name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é€‚åˆSPXå¹³å°"""
        name_lower = name.lower()
        
        # æ’é™¤ä¸é€‚åˆçš„å†…å®¹
        if any(keyword in name_lower for keyword in self.exclude_keywords):
            return False
        
        # æ’é™¤æ˜æ˜¾çš„éç´ æå†…å®¹
        exclude_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[a-f0-9]{8,}$',  # çœ‹èµ·æ¥åƒhash
            r'(test|debug|temp|tmp)',  # æµ‹è¯•ç”¨å†…å®¹
            r'(error|exception|null|undefined)',  # é”™è¯¯ç›¸å…³
            r'(admin|user|login|password)',  # ç³»ç»Ÿç›¸å…³
            r'(http|www|\.com|\.org)',  # ç½‘å€ç›¸å…³
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, name_lower):
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¸¸æˆç´ æç›¸å…³å…³é”®è¯
        game_asset_keywords = [
            # ä¸­æ–‡å…³é”®è¯
            'è§’è‰²', 'èƒŒæ™¯', 'é“å…·', 'å¹³å°', 'å›¾æ ‡', 'æŒ‰é’®', 'ç•Œé¢', 'éŸ³æ•ˆ', 'åŠ¨ç”»', 'ç‰¹æ•ˆ',
            'åƒç´ ', 'å¡é€š', 'åŠ¨æ¼«', 'æ¸¸æˆ', 'ç²¾çµ', 'æ€ªç‰©', 'è‹±é›„', 'æ­¦å™¨', 'å®çŸ³', 'é‡‘å¸',
            'æ£®æ—', 'åŸå¸‚', 'å¤ªç©º', 'æµ·æ´‹', 'å±±è„‰', 'å»ºç­‘', 'æˆ¿å±‹', 'æ ‘æœ¨', 'äº‘æœµ', 'æ˜Ÿæ˜Ÿ',
            
            # è‹±æ–‡å…³é”®è¯
            'character', 'sprite', 'hero', 'player', 'enemy', 'monster', 'npc',
            'background', 'backdrop', 'scene', 'level', 'stage', 'world', 'map',
            'item', 'prop', 'tool', 'weapon', 'armor', 'coin', 'gem', 'key',
            'platform', 'tile', 'block', 'ground', 'wall', 'obstacle',
            'ui', 'button', 'icon', 'menu', 'hud', 'interface',
            'pixel', 'cartoon', 'anime', 'game', 'gaming',
            'forest', 'city', 'space', 'ocean', 'mountain', 'building', 'house'
        ]
        
        has_game_keyword = any(keyword in name_lower for keyword in game_asset_keywords)
        
        # æˆ–è€…åç§°ç»“æ„çœ‹èµ·æ¥åƒç´ æåç§°
        looks_like_asset = any([
            'background' in name_lower,
            'character' in name_lower,
            re.search(r'\w+\s+(sprite|icon|tile|background|character)', name_lower),
            len(name.split()) >= 2 and any(word in name_lower for word in ['pixel', 'cartoon', '2d'])
        ])
        
        return has_game_keyword or looks_like_asset

    def _normalize_name(self, name: str) -> str:
        """è§„èŒƒåŒ–åç§°"""
        # ç»Ÿä¸€å¸¸è§è¯æ±‡
        replacements = {
            # è‹±æ–‡è§„èŒƒåŒ–
            'bg': 'background',
            'char': 'character',
            'btn': 'button',
            'ico': 'icon',
            'img': 'image',
            
            # é£æ ¼è¯æ±‡è§„èŒƒåŒ–
            'pixelart': 'pixel art',
            'pixel-art': 'pixel art',
            '2d': '2D',
            '3d': '3D',
        }
        
        name_lower = name.lower()
        for old, new in replacements.items():
            name_lower = name_lower.replace(old, new)
        
        # é‡æ–°ç»„ç»‡å¤§å°å†™
        words = name_lower.split()
        normalized_words = []
        
        for word in words:
            # ç‰¹æ®Šè¯æ±‡ä¿æŒç‰¹å®šæ ¼å¼
            if word in ['2d', '2D']:
                normalized_words.append('2D')
            elif word in ['3d', '3D']:
                normalized_words.append('3D')
            elif word in ['ui', 'UI']:
                normalized_words.append('UI')
            elif word in ['rpg', 'RPG']:
                normalized_words.append('RPG')
            else:
                # ä¸€èˆ¬è¯æ±‡é¦–å­—æ¯å¤§å†™
                normalized_words.append(word.capitalize())
        
        return ' '.join(normalized_words)

    def _calculate_confidence_score(self, name: str) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        name_lower = name.lower()
        
        # SPXå…³é”®è¯åŒ¹é…åŠ åˆ†
        for keyword in self.spx_keywords:
            if keyword in name_lower:
                score += 0.1
        
        # é•¿åº¦åˆç†æ€§åŠ åˆ†
        if 10 <= len(name) <= 50:
            score += 0.1
        
        # åŒ…å«å¤šä¸ªæœ‰æ„ä¹‰è¯æ±‡åŠ åˆ†
        meaningful_words = len([word for word in name.split() if len(word) > 2])
        if meaningful_words >= 2:
            score += 0.1
        if meaningful_words >= 3:
            score += 0.1
        
        # çœ‹èµ·æ¥åƒä¸“ä¸šå‘½ååŠ åˆ†
        if any(pattern in name_lower for pattern in ['pixel', 'cartoon', 'character', 'background']):
            score += 0.2
        
        return min(score, 1.0)

    def remove_duplicates(self, assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤é¡¹"""
        print("ğŸ”„ Removing duplicates...")
        
        # æŒ‰åç§°å»é‡ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        name_to_asset = {}
        
        for asset in assets:
            name = asset['name']
            if name not in name_to_asset or asset['confidence_score'] > name_to_asset[name]['confidence_score']:
                name_to_asset[name] = asset
        
        unique_assets = list(name_to_asset.values())
        
        print(f"ğŸ—‘ï¸  Removed {len(assets) - len(unique_assets)} duplicates")
        return unique_assets

    def filter_by_quality(self, assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰è´¨é‡ç­›é€‰"""
        print("â­ Filtering by quality...")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        assets.sort(key=lambda x: x['confidence_score'], reverse=True)
        
        # åªä¿ç•™é«˜è´¨é‡çš„ç´ æ
        min_confidence = 0.6
        high_quality_assets = [asset for asset in assets if asset['confidence_score'] >= min_confidence]
        
        # é™åˆ¶æ•°é‡
        max_assets = self.config['output']['max_assets']
        if len(high_quality_assets) > max_assets:
            high_quality_assets = high_quality_assets[:max_assets]
        
        print(f"ğŸ“Š Kept {len(high_quality_assets)} high-quality assets (confidence >= {min_confidence})")
        return high_quality_assets

    def add_spx_enhancements(self, assets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ·»åŠ SPXå¹³å°å¢å¼ºä¿¡æ¯"""
        print("ğŸ® Adding SPX platform enhancements...")
        
        for asset in tqdm(assets, desc="Enhancing"):
            name = asset['name']
            
            # è‡ªåŠ¨åˆ†ç±»
            category = self._auto_categorize(name)
            asset['spx_category'] = category
            
            # ç”Ÿæˆæ ‡ç­¾
            tags = self._generate_tags(name)
            asset['spx_tags'] = tags
            
            # æ·»åŠ å¹³å°é€‚é…è¯´æ˜
            asset['spx_compatible'] = True
            asset['spx_description'] = f"é€‚åˆSPXå¹³å°çš„{category}ç´ æ"
        
        return assets

    def _auto_categorize(self, name: str) -> str:
        """è‡ªåŠ¨åˆ†ç±»"""
        name_lower = name.lower()
        
        # è§’è‰²ç±»
        if any(keyword in name_lower for keyword in ['character', 'hero', 'player', 'enemy', 'monster', 'npc', 'è§’è‰²', 'è‹±é›„', 'æ€ªç‰©']):
            return 'è§’è‰²'
        
        # èƒŒæ™¯ç±»
        elif any(keyword in name_lower for keyword in ['background', 'backdrop', 'scene', 'level', 'world', 'èƒŒæ™¯', 'åœºæ™¯']):
            return 'èƒŒæ™¯'
        
        # é“å…·ç±»
        elif any(keyword in name_lower for keyword in ['item', 'prop', 'tool', 'weapon', 'coin', 'gem', 'é“å…·', 'ç‰©å“', 'æ­¦å™¨', 'é‡‘å¸']):
            return 'é“å…·'
        
        # UIç±»
        elif any(keyword in name_lower for keyword in ['ui', 'button', 'icon', 'menu', 'interface', 'æŒ‰é’®', 'å›¾æ ‡', 'ç•Œé¢']):
            return 'UI'
        
        # å¹³å°ç±»
        elif any(keyword in name_lower for keyword in ['platform', 'tile', 'block', 'ground', 'å¹³å°', 'æ–¹å—']):
            return 'å¹³å°'
        
        else:
            return 'å…¶ä»–'

    def _generate_tags(self, name: str) -> List[str]:
        """ç”Ÿæˆæ ‡ç­¾"""
        tags = []
        name_lower = name.lower()
        
        # é£æ ¼æ ‡ç­¾
        if 'pixel' in name_lower:
            tags.append('åƒç´ ')
        if 'cartoon' in name_lower:
            tags.append('å¡é€š')
        if '2d' in name_lower:
            tags.append('2D')
        
        # é¢œè‰²æ ‡ç­¾
        colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'pink', 'black', 'white']
        for color in colors:
            if color in name_lower:
                tags.append(color)
        
        # ä¸»é¢˜æ ‡ç­¾
        themes = ['forest', 'city', 'space', 'ocean', 'mountain', 'fantasy', 'sci-fi']
        for theme in themes:
            if theme in name_lower:
                tags.append(theme)
        
        return tags

    def generate_statistics(self, assets: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š Generating statistics...")
        
        total_count = len(assets)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        categories = Counter(asset['spx_category'] for asset in assets)
        
        # æŒ‰æ¥æºç»Ÿè®¡
        sources = Counter(asset['source_dataset'] for asset in assets)
        
        # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
        confidence_ranges = {
            '0.9-1.0': 0,
            '0.8-0.9': 0,
            '0.7-0.8': 0,
            '0.6-0.7': 0,
            '0.5-0.6': 0
        }
        
        for asset in assets:
            score = asset['confidence_score']
            if score >= 0.9:
                confidence_ranges['0.9-1.0'] += 1
            elif score >= 0.8:
                confidence_ranges['0.8-0.9'] += 1
            elif score >= 0.7:
                confidence_ranges['0.7-0.8'] += 1
            elif score >= 0.6:
                confidence_ranges['0.6-0.7'] += 1
            else:
                confidence_ranges['0.5-0.6'] += 1
        
        stats = {
            'total_assets': total_count,
            'categories': dict(categories),
            'sources': dict(sources),
            'confidence_distribution': confidence_ranges,
            'average_confidence': sum(asset['confidence_score'] for asset in assets) / total_count if total_count > 0 else 0
        }
        
        return stats

    def save_cleaned_data(self, assets: List[Dict[str, Any]], stats: Dict[str, Any]):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        cleaned_data_path = self.cleaned_dir / "cleaned_assets.json"
        with open(cleaned_data_path, 'w', encoding='utf-8') as f:
            json.dump(assets, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = self.cleaned_dir / "cleaning_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆCSVæ ¼å¼ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
        csv_path = self.cleaned_dir / "cleaned_assets.csv"
        df = pd.DataFrame(assets)
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"âœ… Cleaned data saved:")
        print(f"   ğŸ“„ JSON: {cleaned_data_path}")
        print(f"   ğŸ“Š Stats: {stats_path}")
        print(f"   ğŸ“‹ CSV: {csv_path}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“ˆ Cleaning Summary:")
        print(f"   Total assets: {stats['total_assets']}")
        print(f"   Average confidence: {stats['average_confidence']:.2f}")
        print(f"   Categories: {stats['categories']}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ Starting data cleaning process...")
    
    cleaner = DataCleaner()
    
    # æ­¥éª¤1: åŠ è½½åŸå§‹æ•°æ®
    raw_data = cleaner.load_raw_data()
    if not raw_data:
        return
    
    # æ­¥éª¤2: æ¸…æ´—ç´ æåç§°
    cleaned_assets = cleaner.clean_asset_names(raw_data)
    
    # æ­¥éª¤3: å»é™¤é‡å¤é¡¹
    unique_assets = cleaner.remove_duplicates(cleaned_assets)
    
    # æ­¥éª¤4: æŒ‰è´¨é‡ç­›é€‰
    quality_assets = cleaner.filter_by_quality(unique_assets)
    
    # æ­¥éª¤5: æ·»åŠ SPXå¢å¼ºä¿¡æ¯
    enhanced_assets = cleaner.add_spx_enhancements(quality_assets)
    
    # æ­¥éª¤6: ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = cleaner.generate_statistics(enhanced_assets)
    
    # æ­¥éª¤7: ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    cleaner.save_cleaned_data(enhanced_assets, stats)
    
    print("âœ… Data cleaning completed successfully!")

if __name__ == "__main__":
    main()
