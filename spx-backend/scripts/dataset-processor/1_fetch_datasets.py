#!/usr/bin/env python3
"""
æ­¥éª¤1: ä»GitHubè·å–Awesome Game Datasetsæ•°æ®
"""

import os
import json
import requests
import time
import csv
from pathlib import Path
from typing import List, Dict, Any
from github import Github
from tqdm import tqdm
import pandas as pd

class DatasetFetcher:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.github_token = self.config['github']['token']
        self.repo_name = self.config['github']['awesome_datasets_repo']
        self.rate_limit_delay = self.config['github']['rate_limit_delay']
        
        self.data_dir = Path("data/raw")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        if self.github_token and self.github_token != "your_github_token_here":
            self.github = Github(self.github_token)
        else:
            self.github = Github()  # æ— tokenæ¨¡å¼ï¼Œæœ‰APIé™åˆ¶
            print("âš ï¸  Warning: No GitHub token provided, API rate limits apply")

    def fetch_awesome_datasets_repo(self) -> List[Dict[str, Any]]:
        """è·å–awesome-game-datasetsä»“åº“çš„æ•°æ®é›†ä¿¡æ¯"""
        print("ğŸ” Fetching Awesome Game Datasets repository...")
        
        try:
            repo = self.github.get_repo(self.repo_name)
            readme_content = repo.get_readme().decoded_content.decode('utf-8')
            
            # ä¿å­˜READMEå†…å®¹ç”¨äºåç»­è§£æ
            readme_path = self.data_dir / "awesome_datasets_readme.md"
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            print(f"âœ… README saved to: {readme_path}")
            
            # è§£æREADMEä¸­çš„æ•°æ®é›†é“¾æ¥
            datasets = self._parse_datasets_from_readme(readme_content)
            return datasets
            
        except Exception as e:
            print(f"âŒ Error fetching repository: {e}")
            return []

    def _parse_datasets_from_readme(self, readme_content: str) -> List[Dict[str, Any]]:
        """ä»READMEå†…å®¹ä¸­è§£ææ•°æ®é›†ä¿¡æ¯"""
        datasets = []
        lines = readme_content.split('\n')
        
        current_category = ""
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹åˆ†ç±»æ ‡é¢˜
            if line.startswith('##') and not line.startswith('###'):
                current_category = line.replace('##', '').strip()
                continue
            
            # æ£€æµ‹æ•°æ®é›†é“¾æ¥
            if '[' in line and '](' in line and ('github.com' in line or 'kaggle.com' in line):
                dataset_info = self._extract_dataset_info(line, current_category)
                if dataset_info:
                    datasets.append(dataset_info)
        
        print(f"ğŸ“‹ Found {len(datasets)} datasets in README")
        return datasets

    def _extract_dataset_info(self, line: str, category: str) -> Dict[str, Any]:
        """ä»ä¸€è¡Œæ–‡æœ¬ä¸­æå–æ•°æ®é›†ä¿¡æ¯"""
        try:
            # è§£æmarkdowné“¾æ¥æ ¼å¼ [name](url)
            import re
            
            # åŒ¹é… [text](url) æ ¼å¼
            pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            matches = re.findall(pattern, line)
            
            if not matches:
                return None
            
            name, url = matches[0]
            
            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸ç›¸å…³çš„æ•°æ®é›†
            spx_keywords = self.config['data_filtering']['spx_keywords']
            exclude_keywords = self.config['data_filtering']['exclude_keywords']
            
            name_lower = name.lower()
            line_lower = line.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«SPXç›¸å…³å…³é”®è¯
            has_spx_keyword = any(keyword.lower() in name_lower or keyword.lower() in line_lower 
                                for keyword in spx_keywords)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯
            has_exclude_keyword = any(keyword.lower() in name_lower or keyword.lower() in line_lower 
                                    for keyword in exclude_keywords)
            
            if not has_spx_keyword or has_exclude_keyword:
                return None
            
            return {
                'name': name.strip(),
                'url': url.strip(),
                'category': category,
                'description': line.strip(),
                'source': 'awesome-game-datasets'
            }
            
        except Exception as e:
            print(f"âš ï¸  Error parsing line: {line[:50]}... - {e}")
            return None

    def fetch_individual_datasets(self, datasets: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è·å–å„ä¸ªæ•°æ®é›†çš„è¯¦ç»†å†…å®¹"""
        print("ğŸ“¥ Fetching individual datasets...")
        
        fetched_data = []
        
        for dataset in tqdm(datasets, desc="Downloading datasets"):
            try:
                data = self._fetch_single_dataset(dataset)
                if data:
                    fetched_data.extend(data)
                
                # é˜²æ­¢APIé™åˆ¶
                time.sleep(self.rate_limit_delay)
                
            except Exception as e:
                print(f"âš ï¸  Error fetching {dataset['name']}: {e}")
                continue
        
        print(f"âœ… Successfully fetched data from {len(fetched_data)} entries")
        return fetched_data

    def _fetch_single_dataset(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–å•ä¸ªæ•°æ®é›†çš„å†…å®¹"""
        url = dataset['url']
        name = dataset['name']
        
        # GitHubæ•°æ®é›†å¤„ç†
        if 'github.com' in url:
            return self._fetch_github_dataset(dataset)
        
        # Kaggleæ•°æ®é›†å¤„ç†  
        elif 'kaggle.com' in url:
            return self._fetch_kaggle_dataset(dataset)
        
        # å…¶ä»–CSV/JSONç›´é“¾
        else:
            return self._fetch_direct_dataset(dataset)

    def _fetch_github_dataset(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–GitHubä»“åº“ä¸­çš„æ•°æ®é›†"""
        try:
            url = dataset['url']
            # ä»URLæå–owner/repo
            parts = url.replace('https://github.com/', '').split('/')
            if len(parts) < 2:
                return []
            
            repo_name = f"{parts[0]}/{parts[1]}"
            repo = self.github.get_repo(repo_name)
            
            # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
            data_files = []
            try:
                contents = repo.get_contents("")
                for content in contents:
                    if content.name.endswith(('.csv', '.json', '.txt')) and content.size < 10*1024*1024:  # é™åˆ¶10MB
                        data_files.append(content)
            except:
                pass
            
            # ä¸‹è½½å¹¶è§£ææ•°æ®æ–‡ä»¶
            parsed_data = []
            for file in data_files[:3]:  # é™åˆ¶æ¯ä¸ªä»“åº“æœ€å¤š3ä¸ªæ–‡ä»¶
                try:
                    file_content = file.decoded_content.decode('utf-8')
                    file_data = self._parse_data_file(file_content, file.name, dataset)
                    parsed_data.extend(file_data)
                except:
                    continue
            
            return parsed_data
            
        except Exception as e:
            print(f"âš ï¸  Error fetching GitHub dataset {dataset['name']}: {e}")
            return []

    def _fetch_kaggle_dataset(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–Kaggleæ•°æ®é›†ï¼ˆéœ€è¦API keyï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆKaggle APIï¼Œç›®å‰å…ˆè·³è¿‡
        print(f"â­ï¸  Skipping Kaggle dataset: {dataset['name']} (requires Kaggle API)")
        return []

    def _fetch_direct_dataset(self, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–ç›´é“¾æ•°æ®é›†"""
        try:
            response = requests.get(dataset['url'], timeout=30)
            response.raise_for_status()
            
            file_name = dataset['url'].split('/')[-1]
            return self._parse_data_file(response.text, file_name, dataset)
            
        except Exception as e:
            print(f"âš ï¸  Error fetching direct dataset {dataset['name']}: {e}")
            return []

    def _parse_data_file(self, content: str, filename: str, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææ•°æ®æ–‡ä»¶å†…å®¹"""
        parsed_data = []
        
        try:
            if filename.endswith('.json'):
                data = json.loads(content)
                parsed_data = self._extract_asset_names_from_json(data, dataset)
            
            elif filename.endswith('.csv'):
                # ä¿å­˜CSVæ–‡ä»¶å¹¶ç”¨pandasè¯»å–
                csv_path = self.data_dir / f"{dataset['name']}_{filename}"
                with open(csv_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                df = pd.read_csv(csv_path)
                parsed_data = self._extract_asset_names_from_dataframe(df, dataset)
            
            elif filename.endswith('.txt'):
                lines = content.split('\n')
                parsed_data = self._extract_asset_names_from_text(lines, dataset)
        
        except Exception as e:
            print(f"âš ï¸  Error parsing file {filename}: {e}")
        
        return parsed_data

    def _extract_asset_names_from_json(self, data: Any, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»JSONæ•°æ®ä¸­æå–ç´ æåç§°"""
        asset_names = []
        
        def extract_names(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if any(keyword in key.lower() for keyword in ['name', 'title', 'asset', 'item', 'sprite']):
                        if isinstance(value, str) and self._is_valid_asset_name(value):
                            asset_names.append({
                                'name': value,
                                'source_dataset': dataset['name'],
                                'source_url': dataset['url'],
                                'category': dataset['category'],
                                'path': f"{path}.{key}" if path else key
                            })
                    elif isinstance(value, (dict, list)):
                        extract_names(value, f"{path}.{key}" if path else key)
            
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    extract_names(item, f"{path}[{i}]")
        
        extract_names(data)
        return asset_names

    def _extract_asset_names_from_dataframe(self, df: pd.DataFrame, dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»DataFrameä¸­æå–ç´ æåç§°"""
        asset_names = []
        
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«ç´ æåç§°çš„åˆ—
        name_columns = []
        for col in df.columns:
            if any(keyword in col.lower() for keyword in ['name', 'title', 'asset', 'item', 'sprite']):
                name_columns.append(col)
        
        for col in name_columns:
            for idx, value in df[col].items():
                if pd.notna(value) and isinstance(value, str) and self._is_valid_asset_name(value):
                    asset_names.append({
                        'name': value,
                        'source_dataset': dataset['name'],
                        'source_url': dataset['url'],
                        'category': dataset['category'],
                        'column': col,
                        'row': idx
                    })
        
        return asset_names

    def _extract_asset_names_from_text(self, lines: List[str], dataset: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬è¡Œä¸­æå–ç´ æåç§°"""
        asset_names = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if line and self._is_valid_asset_name(line):
                asset_names.append({
                    'name': line,
                    'source_dataset': dataset['name'],
                    'source_url': dataset['url'],
                    'category': dataset['category'],
                    'line_number': i + 1
                })
        
        return asset_names

    def _is_valid_asset_name(self, name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç´ æåç§°"""
        if not isinstance(name, str):
            return False
        
        name = name.strip()
        min_length = self.config['data_filtering']['min_name_length']
        max_length = self.config['data_filtering']['max_name_length']
        
        if len(name) < min_length or len(name) > max_length:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«SPXç›¸å…³å…³é”®è¯
        spx_keywords = self.config['data_filtering']['spx_keywords']
        exclude_keywords = self.config['data_filtering']['exclude_keywords']
        
        name_lower = name.lower()
        
        # æ’é™¤æ˜æ˜¾ä¸ç›¸å…³çš„å†…å®¹
        if any(keyword.lower() in name_lower for keyword in exclude_keywords):
            return False
        
        # æ’é™¤çº¯æ•°å­—ã€URLç­‰
        if name.isdigit() or 'http' in name_lower or '@' in name:
            return False
        
        return True

    def save_raw_data(self, datasets: List[Dict[str, Any]], fetched_data: List[Dict[str, Any]]):
        """ä¿å­˜åŸå§‹æ•°æ®"""
        # ä¿å­˜æ•°æ®é›†åˆ—è¡¨
        datasets_path = self.data_dir / "datasets_list.json"
        with open(datasets_path, 'w', encoding='utf-8') as f:
            json.dump(datasets, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è·å–çš„æ•°æ®
        raw_data_path = self.data_dir / "raw_asset_data.json"
        with open(raw_data_path, 'w', encoding='utf-8') as f:
            json.dump(fetched_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Raw data saved:")
        print(f"   ğŸ“‹ Datasets list: {datasets_path}")
        print(f"   ğŸ“„ Asset data: {raw_data_path}")
        print(f"   ğŸ“Š Total assets found: {len(fetched_data)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting dataset fetching process...")
    
    fetcher = DatasetFetcher()
    
    # æ­¥éª¤1: è·å–awesome-game-datasetsä»“åº“ä¿¡æ¯
    datasets = fetcher.fetch_awesome_datasets_repo()
    if not datasets:
        print("âŒ No datasets found in repository")
        return
    
    # æ­¥éª¤2: è·å–å„ä¸ªæ•°æ®é›†çš„å†…å®¹
    fetched_data = fetcher.fetch_individual_datasets(datasets)
    
    # æ­¥éª¤3: ä¿å­˜åŸå§‹æ•°æ®
    fetcher.save_raw_data(datasets, fetched_data)
    
    print("âœ… Dataset fetching completed successfully!")

if __name__ == "__main__":
    main()
